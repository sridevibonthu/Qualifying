{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mynetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sridevibonthu/Qualifying/blob/master/Mynetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0FelYEaQroJ",
        "colab_type": "code",
        "outputId": "f9e852c7-4885-4743-c20f-06ede7325e6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A96OD_jSQ__g",
        "colab_type": "code",
        "outputId": "a86c403a-9b11-4527-e3ec-8d4215b293d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!unzip -q \"/content/gdrive/My Drive/Assignment5/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "replace resized/9733.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm8fugntRE4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TIyvOKF8RLv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Assignment5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnRkZy1KRNp-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import clr\n",
        "import cutout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlGq1CBDRXyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOTsYGgHIjjO",
        "colab_type": "code",
        "outputId": "a31df3b9-043d-41bc-a6a3-f66dc7787e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Di9COusRZ9d",
        "colab_type": "code",
        "outputId": "5bf1a1e5-1d6b-439f-ecc6-e7731ab4334e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eetMVqx1SPXH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentation=None):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augmentation = augmentation\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        if self.augmentation is not None:\n",
        "          image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "          target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "          }\n",
        "        else:\n",
        "          image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "          #self.augmentation.fit(image)\n",
        "          #image = self.augmentation.flow(image)\n",
        "          target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "          }\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm1f2E7OTPoY",
        "colab_type": "code",
        "outputId": "5ecf862a-a770-4681-d9e0-36c43cfa3785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T94TBx6vTWfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pixel_level=False\n",
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32, augmentation=ImageDataGenerator(horizontal_flip=True, preprocessing_function=cutout.get_random_eraser(v_l=0, v_h=1, pixel_level=pixel_level)))\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wlO6I5zTljv",
        "colab_type": "code",
        "outputId": "248db311-3aa3-4695-eded-40a87a525891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSTIhcTqT0al",
        "colab_type": "code",
        "outputId": "b3fa7cd7-4822-48b0-b2aa-90cef17ef726",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from keras.applications import ResNet50\n",
        "backbone = ResNet50(\n",
        "    weights=\"imagenet\", \n",
        "    include_top=False, \n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
            "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4GQsY22T6JE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neck = backbone.output\n",
        "neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hXxgTg4T9kF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e116UZ5xUAf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# freeze backbone\n",
        "for layer in backbone.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuf_-m0uUD6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare model model saving directory.\n",
        "import os\n",
        "model_type = \"ResNet50\"\n",
        "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
        "model_name = 'personattributes_%s_model.{epoch:03d}.h5' % model_type\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "filepath = os.path.join(save_dir, model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAkKhNjPUI1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "from clr import CyclicLR\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "clr1 = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
        "                        step_size=2000., mode='triangular2')\n",
        "\n",
        "\n",
        "\n",
        "callbacks = [checkpoint, clr1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8sGYtWuULXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "\n",
        "\n",
        "opt = SGD(lr=0.1, momentum=0.9)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5YCHbbSUPyk",
        "colab_type": "code",
        "outputId": "70b0ae1e-11ed-464f-e13a-201c3f4691e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=24,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/24\n",
            "  4/360 [..............................] - ETA: 6:43 - loss: 15.1480 - gender_output_loss: 1.2894 - image_quality_output_loss: 1.8123 - age_output_loss: 2.8470 - weight_output_loss: 1.3729 - bag_output_loss: 1.5292 - footwear_output_loss: 2.0325 - pose_output_loss: 2.1739 - emotion_output_loss: 2.0909 - gender_output_acc: 0.4688 - image_quality_output_acc: 0.4453 - age_output_acc: 0.2109 - weight_output_acc: 0.5156 - bag_output_acc: 0.5000 - footwear_output_acc: 0.3281 - pose_output_acc: 0.4531 - emotion_output_acc: 0.4531"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:95: RuntimeWarning: Method (on_train_batch_end) is slow compared to the batch update (0.240367). Check your callbacks.\n",
            "  % (hook_name, delta_t_median), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "359/360 [============================>.] - ETA: 0s - loss: 8.2667 - gender_output_loss: 0.6259 - image_quality_output_loss: 1.0482 - age_output_loss: 1.5315 - weight_output_loss: 1.0913 - bag_output_loss: 0.9830 - footwear_output_loss: 1.0209 - pose_output_loss: 0.9266 - emotion_output_loss: 1.0392 - gender_output_acc: 0.6567 - image_quality_output_acc: 0.5264 - age_output_acc: 0.3638 - weight_output_acc: 0.6181 - bag_output_acc: 0.5555 - footwear_output_acc: 0.5389 - pose_output_acc: 0.6048 - emotion_output_acc: 0.6997Epoch 1/24\n",
            "360/360 [==============================] - 48s 133ms/step - loss: 8.2660 - gender_output_loss: 0.6258 - image_quality_output_loss: 1.0484 - age_output_loss: 1.5308 - weight_output_loss: 1.0910 - bag_output_loss: 0.9831 - footwear_output_loss: 1.0215 - pose_output_loss: 0.9268 - emotion_output_loss: 1.0386 - gender_output_acc: 0.6568 - image_quality_output_acc: 0.5262 - age_output_acc: 0.3644 - weight_output_acc: 0.6185 - bag_output_acc: 0.5555 - footwear_output_acc: 0.5385 - pose_output_acc: 0.6047 - emotion_output_acc: 0.6998 - val_loss: 7.8228 - val_gender_output_loss: 0.5745 - val_image_quality_output_loss: 1.0169 - val_age_output_loss: 1.4718 - val_weight_output_loss: 1.0524 - val_bag_output_loss: 0.9235 - val_footwear_output_loss: 0.9460 - val_pose_output_loss: 0.9035 - val_emotion_output_loss: 0.9341 - val_gender_output_acc: 0.7283 - val_image_quality_output_acc: 0.5413 - val_age_output_acc: 0.4047 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5444 - val_footwear_output_acc: 0.5927 - val_pose_output_acc: 0.6668 - val_emotion_output_acc: 0.7132\n",
            "Epoch 2/24"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
            "  'skipping.' % (self.monitor), RuntimeWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.7187 - gender_output_loss: 0.6014 - image_quality_output_loss: 0.9912 - age_output_loss: 1.4598 - weight_output_loss: 1.0119 - bag_output_loss: 0.9132 - footwear_output_loss: 0.9558 - pose_output_loss: 0.8571 - emotion_output_loss: 0.9283 - gender_output_acc: 0.6451 - image_quality_output_acc: 0.5508 - age_output_acc: 0.3836 - weight_output_acc: 0.6339 - bag_output_acc: 0.5765 - footwear_output_acc: 0.5564 - pose_output_acc: 0.6387 - emotion_output_acc: 0.7085\n",
            "360/360 [==============================] - 43s 120ms/step - loss: 7.7198 - gender_output_loss: 0.6016 - image_quality_output_loss: 0.9913 - age_output_loss: 1.4597 - weight_output_loss: 1.0120 - bag_output_loss: 0.9132 - footwear_output_loss: 0.9561 - pose_output_loss: 0.8572 - emotion_output_loss: 0.9287 - gender_output_acc: 0.6449 - image_quality_output_acc: 0.5507 - age_output_acc: 0.3838 - weight_output_acc: 0.6338 - bag_output_acc: 0.5766 - footwear_output_acc: 0.5557 - pose_output_acc: 0.6385 - emotion_output_acc: 0.7082 - val_loss: 7.7644 - val_gender_output_loss: 0.6337 - val_image_quality_output_loss: 0.9922 - val_age_output_loss: 1.4365 - val_weight_output_loss: 1.0073 - val_bag_output_loss: 0.9185 - val_footwear_output_loss: 1.0030 - val_pose_output_loss: 0.8733 - val_emotion_output_loss: 0.9000 - val_gender_output_acc: 0.5595 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.4083 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5489 - val_footwear_output_acc: 0.5086 - val_pose_output_acc: 0.6462 - val_emotion_output_acc: 0.7132\n",
            "Epoch 3/24\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.6424 - gender_output_loss: 0.6031 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4431 - weight_output_loss: 0.9943 - bag_output_loss: 0.9037 - footwear_output_loss: 0.9557 - pose_output_loss: 0.8523 - emotion_output_loss: 0.9096 - gender_output_acc: 0.6409 - image_quality_output_acc: 0.5549 - age_output_acc: 0.3894 - weight_output_acc: 0.6341 - bag_output_acc: 0.5751 - footwear_output_acc: 0.5513 - pose_output_acc: 0.6478 - emotion_output_acc: 0.7068Epoch 3/24\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 7.6424 - gender_output_loss: 0.6029 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4428 - weight_output_loss: 0.9942 - bag_output_loss: 0.9040 - footwear_output_loss: 0.9557 - pose_output_loss: 0.8527 - emotion_output_loss: 0.9097 - gender_output_acc: 0.6412 - image_quality_output_acc: 0.5549 - age_output_acc: 0.3899 - weight_output_acc: 0.6342 - bag_output_acc: 0.5749 - footwear_output_acc: 0.5511 - pose_output_acc: 0.6476 - emotion_output_acc: 0.7068 - val_loss: 7.8865 - val_gender_output_loss: 0.5952 - val_image_quality_output_loss: 0.9929 - val_age_output_loss: 1.4322 - val_weight_output_loss: 1.0085 - val_bag_output_loss: 0.9432 - val_footwear_output_loss: 0.9586 - val_pose_output_loss: 1.0390 - val_emotion_output_loss: 0.9170 - val_gender_output_acc: 0.6129 - val_image_quality_output_acc: 0.5101 - val_age_output_acc: 0.4068 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5484 - val_footwear_output_acc: 0.5454 - val_pose_output_acc: 0.6134 - val_emotion_output_acc: 0.7132\n",
            "Epoch 4/24\n",
            "360/360 [==============================] - 42s 118ms/step - loss: 7.6858 - gender_output_loss: 0.6210 - image_quality_output_loss: 0.9807 - age_output_loss: 1.4416 - weight_output_loss: 0.9948 - bag_output_loss: 0.9101 - footwear_output_loss: 0.9601 - pose_output_loss: 0.8688 - emotion_output_loss: 0.9088 - gender_output_acc: 0.6415 - image_quality_output_acc: 0.5562 - age_output_acc: 0.3903 - weight_output_acc: 0.6351 - bag_output_acc: 0.5764 - footwear_output_acc: 0.5503 - pose_output_acc: 0.6434 - emotion_output_acc: 0.7074 - val_loss: 7.6670 - val_gender_output_loss: 0.6073 - val_image_quality_output_loss: 0.9913 - val_age_output_loss: 1.4335 - val_weight_output_loss: 1.0054 - val_bag_output_loss: 0.9108 - val_footwear_output_loss: 0.9529 - val_pose_output_loss: 0.8615 - val_emotion_output_loss: 0.9043 - val_gender_output_acc: 0.6285 - val_image_quality_output_acc: 0.5418 - val_age_output_acc: 0.3982 - val_weight_output_acc: 0.6169 - val_bag_output_acc: 0.5595 - val_footwear_output_acc: 0.5433 - val_pose_output_acc: 0.6457 - val_emotion_output_acc: 0.7026\n",
            "Epoch 5/24\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.6272 - gender_output_loss: 0.6043 - image_quality_output_loss: 0.9802 - age_output_loss: 1.4360 - weight_output_loss: 0.9949 - bag_output_loss: 0.9016 - footwear_output_loss: 0.9539 - pose_output_loss: 0.8492 - emotion_output_loss: 0.9072 - gender_output_acc: 0.6575 - image_quality_output_acc: 0.5560 - age_output_acc: 0.3949 - weight_output_acc: 0.6346 - bag_output_acc: 0.5799 - footwear_output_acc: 0.5511 - pose_output_acc: 0.6528 - emotion_output_acc: 0.7093Epoch 5/24\n",
            "360/360 [==============================] - 43s 119ms/step - loss: 7.6269 - gender_output_loss: 0.6044 - image_quality_output_loss: 0.9806 - age_output_loss: 1.4358 - weight_output_loss: 0.9947 - bag_output_loss: 0.9017 - footwear_output_loss: 0.9541 - pose_output_loss: 0.8487 - emotion_output_loss: 0.9071 - gender_output_acc: 0.6576 - image_quality_output_acc: 0.5556 - age_output_acc: 0.3949 - weight_output_acc: 0.6345 - bag_output_acc: 0.5798 - footwear_output_acc: 0.5511 - pose_output_acc: 0.6530 - emotion_output_acc: 0.7092 - val_loss: 7.6742 - val_gender_output_loss: 0.5751 - val_image_quality_output_loss: 0.9982 - val_age_output_loss: 1.4130 - val_weight_output_loss: 1.0257 - val_bag_output_loss: 0.9360 - val_footwear_output_loss: 0.9040 - val_pose_output_loss: 0.9207 - val_emotion_output_loss: 0.9015 - val_gender_output_acc: 0.6870 - val_image_quality_output_acc: 0.5413 - val_age_output_acc: 0.4098 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5610 - val_footwear_output_acc: 0.6008 - val_pose_output_acc: 0.6300 - val_emotion_output_acc: 0.7137\n",
            "Epoch 6/24\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 7.5158 - gender_output_loss: 0.5884 - image_quality_output_loss: 0.9755 - age_output_loss: 1.4248 - weight_output_loss: 0.9873 - bag_output_loss: 0.8946 - footwear_output_loss: 0.9204 - pose_output_loss: 0.8284 - emotion_output_loss: 0.8962 - gender_output_acc: 0.6709 - image_quality_output_acc: 0.5594 - age_output_acc: 0.3958 - weight_output_acc: 0.6372 - bag_output_acc: 0.5844 - footwear_output_acc: 0.5760 - pose_output_acc: 0.6633 - emotion_output_acc: 0.7094 - val_loss: 7.6357 - val_gender_output_loss: 0.6383 - val_image_quality_output_loss: 0.9798 - val_age_output_loss: 1.4126 - val_weight_output_loss: 1.0033 - val_bag_output_loss: 0.9199 - val_footwear_output_loss: 0.8962 - val_pose_output_loss: 0.8906 - val_emotion_output_loss: 0.8950 - val_gender_output_acc: 0.6094 - val_image_quality_output_acc: 0.5439 - val_age_output_acc: 0.4093 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5575 - val_footwear_output_acc: 0.6064 - val_pose_output_acc: 0.6240 - val_emotion_output_acc: 0.7137\n",
            "Epoch 7/24\n",
            "360/360 [==============================] - 43s 118ms/step - loss: 7.3541 - gender_output_loss: 0.5523 - image_quality_output_loss: 0.9671 - age_output_loss: 1.4166 - weight_output_loss: 0.9792 - bag_output_loss: 0.8798 - footwear_output_loss: 0.8762 - pose_output_loss: 0.7959 - emotion_output_loss: 0.8871 - gender_output_acc: 0.7122 - image_quality_output_acc: 0.5582 - age_output_acc: 0.3954 - weight_output_acc: 0.6397 - bag_output_acc: 0.6003 - footwear_output_acc: 0.6072 - pose_output_acc: 0.6730 - emotion_output_acc: 0.7097 - val_loss: 7.4983 - val_gender_output_loss: 0.5618 - val_image_quality_output_loss: 0.9778 - val_age_output_loss: 1.4120 - val_weight_output_loss: 1.0038 - val_bag_output_loss: 0.9081 - val_footwear_output_loss: 0.8828 - val_pose_output_loss: 0.8596 - val_emotion_output_loss: 0.8924 - val_gender_output_acc: 0.6966 - val_image_quality_output_acc: 0.5454 - val_age_output_acc: 0.4068 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5660 - val_footwear_output_acc: 0.6104 - val_pose_output_acc: 0.6457 - val_emotion_output_acc: 0.7132\n",
            "\n",
            "Epoch 8/24\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.1151 - gender_output_loss: 0.4820 - image_quality_output_loss: 0.9549 - age_output_loss: 1.4058 - weight_output_loss: 0.9693 - bag_output_loss: 0.8694 - footwear_output_loss: 0.8307 - pose_output_loss: 0.7292 - emotion_output_loss: 0.8738 - gender_output_acc: 0.7698 - image_quality_output_acc: 0.5675 - age_output_acc: 0.4021 - weight_output_acc: 0.6402 - bag_output_acc: 0.6048 - footwear_output_acc: 0.6321 - pose_output_acc: 0.6936 - emotion_output_acc: 0.7116Epoch 8/24\n",
            "360/360 [==============================] - 42s 118ms/step - loss: 7.1162 - gender_output_loss: 0.4818 - image_quality_output_loss: 0.9547 - age_output_loss: 1.4062 - weight_output_loss: 0.9698 - bag_output_loss: 0.8691 - footwear_output_loss: 0.8306 - pose_output_loss: 0.7291 - emotion_output_loss: 0.8749 - gender_output_acc: 0.7698 - image_quality_output_acc: 0.5675 - age_output_acc: 0.4019 - weight_output_acc: 0.6401 - bag_output_acc: 0.6049 - footwear_output_acc: 0.6322 - pose_output_acc: 0.6935 - emotion_output_acc: 0.7111 - val_loss: 7.4128 - val_gender_output_loss: 0.5496 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.4010 - val_weight_output_loss: 1.0034 - val_bag_output_loss: 0.9211 - val_footwear_output_loss: 0.8378 - val_pose_output_loss: 0.8314 - val_emotion_output_loss: 0.8948 - val_gender_output_acc: 0.7021 - val_image_quality_output_acc: 0.5484 - val_age_output_acc: 0.4073 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5529 - val_footwear_output_acc: 0.6366 - val_pose_output_acc: 0.6406 - val_emotion_output_acc: 0.7142\n",
            "Epoch 9/24\n",
            "360/360 [==============================] - 42s 118ms/step - loss: 7.1162 - gender_output_loss: 0.4818 - image_quality_output_loss: 0.9547 - age_output_loss: 1.4062 - weight_output_loss: 0.9698 - bag_output_loss: 0.8691 - footwear_output_loss: 0.8306 - pose_output_loss: 0.7291 - emotion_output_loss: 0.8749 - gender_output_acc: 0.7698 - image_quality_output_acc: 0.5675 - age_output_acc: 0.4019 - weight_output_acc: 0.6401 - bag_output_acc: 0.6049 - footwear_output_acc: 0.6322 - pose_output_acc: 0.6935 - emotion_output_acc: 0.7111 - val_loss: 7.4128 - val_gender_output_loss: 0.5496 - val_image_quality_output_loss: 0.9736 - val_age_output_loss: 1.4010 - val_weight_output_loss: 1.0034 - val_bag_output_loss: 0.9211 - val_footwear_output_loss: 0.8378 - val_pose_output_loss: 0.8314 - val_emotion_output_loss: 0.8948 - val_gender_output_acc: 0.7021 - val_image_quality_output_acc: 0.5484 - val_age_output_acc: 0.4073 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5529 - val_footwear_output_acc: 0.6366 - val_pose_output_acc: 0.6406 - val_emotion_output_acc: 0.7142\n",
            "360/360 [==============================] - 42s 118ms/step - loss: 6.8678 - gender_output_loss: 0.4238 - image_quality_output_loss: 0.9370 - age_output_loss: 1.3816 - weight_output_loss: 0.9597 - bag_output_loss: 0.8500 - footwear_output_loss: 0.7818 - pose_output_loss: 0.6763 - emotion_output_loss: 0.8576 - gender_output_acc: 0.8039 - image_quality_output_acc: 0.5737 - age_output_acc: 0.4030 - weight_output_acc: 0.6419 - bag_output_acc: 0.6200 - footwear_output_acc: 0.6545 - pose_output_acc: 0.7139 - emotion_output_acc: 0.7132 - val_loss: 7.2843 - val_gender_output_loss: 0.4886 - val_image_quality_output_loss: 0.9623 - val_age_output_loss: 1.3931 - val_weight_output_loss: 0.9993 - val_bag_output_loss: 0.8907 - val_footwear_output_loss: 0.8424 - val_pose_output_loss: 0.8219 - val_emotion_output_loss: 0.8860 - val_gender_output_acc: 0.7566 - val_image_quality_output_acc: 0.5509 - val_age_output_acc: 0.4138 - val_weight_output_acc: 0.6139 - val_bag_output_acc: 0.6008 - val_footwear_output_acc: 0.6225 - val_pose_output_acc: 0.6492 - val_emotion_output_acc: 0.7137\n",
            "Epoch 10/24\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 6.6289 - gender_output_loss: 0.3737 - image_quality_output_loss: 0.9078 - age_output_loss: 1.3661 - weight_output_loss: 0.9462 - bag_output_loss: 0.8339 - footwear_output_loss: 0.7472 - pose_output_loss: 0.6085 - emotion_output_loss: 0.8454 - gender_output_acc: 0.8297 - image_quality_output_acc: 0.5890 - age_output_acc: 0.4123 - weight_output_acc: 0.6425 - bag_output_acc: 0.6243 - footwear_output_acc: 0.6722 - pose_output_acc: 0.7390 - emotion_output_acc: 0.7140 - val_loss: 7.3370 - val_gender_output_loss: 0.5607 - val_image_quality_output_loss: 0.9549 - val_age_output_loss: 1.3987 - val_weight_output_loss: 0.9950 - val_bag_output_loss: 0.8937 - val_footwear_output_loss: 0.8253 - val_pose_output_loss: 0.8221 - val_emotion_output_loss: 0.8865 - val_gender_output_acc: 0.7172 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.4073 - val_weight_output_acc: 0.6200 - val_bag_output_acc: 0.5610 - val_footwear_output_acc: 0.6482 - val_pose_output_acc: 0.6467 - val_emotion_output_acc: 0.7142\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\bEpoch 11/24\n",
            "360/360 [==============================] - 43s 118ms/step - loss: 6.3720 - gender_output_loss: 0.3343 - image_quality_output_loss: 0.8707 - age_output_loss: 1.3373 - weight_output_loss: 0.9292 - bag_output_loss: 0.8120 - footwear_output_loss: 0.7046 - pose_output_loss: 0.5536 - emotion_output_loss: 0.8304 - gender_output_acc: 0.8496 - image_quality_output_acc: 0.6010 - age_output_acc: 0.4185 - weight_output_acc: 0.6487 - bag_output_acc: 0.6403 - footwear_output_acc: 0.6875 - pose_output_acc: 0.7668 - emotion_output_acc: 0.7161 - val_loss: 7.2490 - val_gender_output_loss: 0.5196 - val_image_quality_output_loss: 0.9543 - val_age_output_loss: 1.3933 - val_weight_output_loss: 1.0009 - val_bag_output_loss: 0.8972 - val_footwear_output_loss: 0.8207 - val_pose_output_loss: 0.7823 - val_emotion_output_loss: 0.8808 - val_gender_output_acc: 0.7409 - val_image_quality_output_acc: 0.5504 - val_age_output_acc: 0.4143 - val_weight_output_acc: 0.6169 - val_bag_output_acc: 0.5892 - val_footwear_output_acc: 0.6482 - val_pose_output_acc: 0.6719 - val_emotion_output_acc: 0.7142\n",
            "Epoch 11/24\n",
            "Epoch 12/24\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.1711 - gender_output_loss: 0.3026 - image_quality_output_loss: 0.8464 - age_output_loss: 1.3163 - weight_output_loss: 0.9102 - bag_output_loss: 0.7970 - footwear_output_loss: 0.6788 - pose_output_loss: 0.5012 - emotion_output_loss: 0.8186 - gender_output_acc: 0.8675 - image_quality_output_acc: 0.6149 - age_output_acc: 0.4237 - weight_output_acc: 0.6536 - bag_output_acc: 0.6433 - footwear_output_acc: 0.7006 - pose_output_acc: 0.7940 - emotion_output_acc: 0.7176Epoch 12/24\n",
            "360/360 [==============================] - 42s 118ms/step - loss: 6.1717 - gender_output_loss: 0.3023 - image_quality_output_loss: 0.8471 - age_output_loss: 1.3166 - weight_output_loss: 0.9106 - bag_output_loss: 0.7969 - footwear_output_loss: 0.6786 - pose_output_loss: 0.5012 - emotion_output_loss: 0.8185 - gender_output_acc: 0.8677 - image_quality_output_acc: 0.6145 - age_output_acc: 0.4237 - weight_output_acc: 0.6535 - bag_output_acc: 0.6433 - footwear_output_acc: 0.7008 - pose_output_acc: 0.7940 - emotion_output_acc: 0.7175 - val_loss: 7.3613 - val_gender_output_loss: 0.5771 - val_image_quality_output_loss: 0.9579 - val_age_output_loss: 1.4059 - val_weight_output_loss: 1.0009 - val_bag_output_loss: 0.9090 - val_footwear_output_loss: 0.8373 - val_pose_output_loss: 0.7932 - val_emotion_output_loss: 0.8801 - val_gender_output_acc: 0.7243 - val_image_quality_output_acc: 0.5559 - val_age_output_acc: 0.4093 - val_weight_output_acc: 0.6179 - val_bag_output_acc: 0.5842 - val_footwear_output_acc: 0.6386 - val_pose_output_acc: 0.6653 - val_emotion_output_acc: 0.7142\n",
            "Epoch 13/24\n",
            "359/360 [============================>.] - ETA: 0s - loss: 6.0409 - gender_output_loss: 0.2916 - image_quality_output_loss: 0.8276 - age_output_loss: 1.3044 - weight_output_loss: 0.9017 - bag_output_loss: 0.7861 - footwear_output_loss: 0.6556 - pose_output_loss: 0.4617 - emotion_output_loss: 0.8123 - gender_output_acc: 0.8679 - image_quality_output_acc: 0.6272 - age_output_acc: 0.4307 - weight_output_acc: 0.6559 - bag_output_acc: 0.6536 - footwear_output_acc: 0.7142 - pose_output_acc: 0.8145 - emotion_output_acc: 0.7194\n",
            "360/360 [==============================] - 42s 118ms/step - loss: 6.0421 - gender_output_loss: 0.2915 - image_quality_output_loss: 0.8276 - age_output_loss: 1.3049 - weight_output_loss: 0.9021 - bag_output_loss: 0.7862 - footwear_output_loss: 0.6557 - pose_output_loss: 0.4620 - emotion_output_loss: 0.8121 - gender_output_acc: 0.8681 - image_quality_output_acc: 0.6271 - age_output_acc: 0.4305 - weight_output_acc: 0.6556 - bag_output_acc: 0.6534 - footwear_output_acc: 0.7143 - pose_output_acc: 0.8144 - emotion_output_acc: 0.7194 - val_loss: 7.3490 - val_gender_output_loss: 0.5649 - val_image_quality_output_loss: 0.9514 - val_age_output_loss: 1.3763 - val_weight_output_loss: 0.9989 - val_bag_output_loss: 0.8933 - val_footwear_output_loss: 0.8417 - val_pose_output_loss: 0.8185 - val_emotion_output_loss: 0.9040 - val_gender_output_acc: 0.7026 - val_image_quality_output_acc: 0.5554 - val_age_output_acc: 0.4103 - val_weight_output_acc: 0.6184 - val_bag_output_acc: 0.5756 - val_footwear_output_acc: 0.6426 - val_pose_output_acc: 0.6678 - val_emotion_output_acc: 0.7142\n",
            "Epoch 14/24\n",
            "360/360 [==============================] - 43s 118ms/step - loss: 5.9457 - gender_output_loss: 0.2797 - image_quality_output_loss: 0.8165 - age_output_loss: 1.2904 - weight_output_loss: 0.8905 - bag_output_loss: 0.7818 - footwear_output_loss: 0.6498 - pose_output_loss: 0.4272 - emotion_output_loss: 0.8098 - gender_output_acc: 0.8763 - image_quality_output_acc: 0.6328 - age_output_acc: 0.4373 - weight_output_acc: 0.6574 - bag_output_acc: 0.6539 - footwear_output_acc: 0.7159 - pose_output_acc: 0.8352 - emotion_output_acc: 0.7187 - val_loss: 7.1507 - val_gender_output_loss: 0.4976 - val_image_quality_output_loss: 0.9459 - val_age_output_loss: 1.3800 - val_weight_output_loss: 1.0107 - val_bag_output_loss: 0.8807 - val_footwear_output_loss: 0.8266 - val_pose_output_loss: 0.7346 - val_emotion_output_loss: 0.8746 - val_gender_output_acc: 0.7475 - val_image_quality_output_acc: 0.5575 - val_age_output_acc: 0.4194 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5993 - val_footwear_output_acc: 0.6436 - val_pose_output_acc: 0.7036 - val_emotion_output_acc: 0.7137\n",
            "Epoch 15/24\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 5.8198 - gender_output_loss: 0.2666 - image_quality_output_loss: 0.8031 - age_output_loss: 1.2737 - weight_output_loss: 0.8754 - bag_output_loss: 0.7643 - footwear_output_loss: 0.6375 - pose_output_loss: 0.3989 - emotion_output_loss: 0.8002 - gender_output_acc: 0.8799 - image_quality_output_acc: 0.6462 - age_output_acc: 0.4381 - weight_output_acc: 0.6626 - bag_output_acc: 0.6651 - footwear_output_acc: 0.7148 - pose_output_acc: 0.8502 - emotion_output_acc: 0.7205 - val_loss: 7.3951 - val_gender_output_loss: 0.5484 - val_image_quality_output_loss: 0.9421 - val_age_output_loss: 1.4130 - val_weight_output_loss: 1.0534 - val_bag_output_loss: 0.9371 - val_footwear_output_loss: 0.8598 - val_pose_output_loss: 0.7643 - val_emotion_output_loss: 0.8770 - val_gender_output_acc: 0.7349 - val_image_quality_output_acc: 0.5494 - val_age_output_acc: 0.4158 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5630 - val_footwear_output_acc: 0.6058 - val_pose_output_acc: 0.6966 - val_emotion_output_acc: 0.7142\n",
            "Epoch 15/24\n",
            "Epoch 16/24\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.7732 - gender_output_loss: 0.2716 - image_quality_output_loss: 0.7928 - age_output_loss: 1.2742 - weight_output_loss: 0.8645 - bag_output_loss: 0.7499 - footwear_output_loss: 0.6385 - pose_output_loss: 0.3828 - emotion_output_loss: 0.7990 - gender_output_acc: 0.8831 - image_quality_output_acc: 0.6517 - age_output_acc: 0.4424 - weight_output_acc: 0.6660 - bag_output_acc: 0.6722 - footwear_output_acc: 0.7177 - pose_output_acc: 0.8538 - emotion_output_acc: 0.7214 - val_loss: 7.4092 - val_gender_output_loss: 0.5763 - val_image_quality_output_loss: 0.9735 - val_age_output_loss: 1.4217 - val_weight_output_loss: 1.0134 - val_bag_output_loss: 0.9130 - val_footwear_output_loss: 0.8506 - val_pose_output_loss: 0.7725 - val_emotion_output_loss: 0.8881 - val_gender_output_acc: 0.7319 - val_image_quality_output_acc: 0.5469 - val_age_output_acc: 0.4047 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.6023 - val_footwear_output_acc: 0.6462 - val_pose_output_acc: 0.7001 - val_emotion_output_acc: 0.7132\n",
            "Epoch 17/24\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 5.7153 - gender_output_loss: 0.2687 - image_quality_output_loss: 0.7771 - age_output_loss: 1.2555 - weight_output_loss: 0.8564 - bag_output_loss: 0.7425 - footwear_output_loss: 0.6407 - pose_output_loss: 0.3744 - emotion_output_loss: 0.7999 - gender_output_acc: 0.8818 - image_quality_output_acc: 0.6582 - age_output_acc: 0.4489 - weight_output_acc: 0.6656 - bag_output_acc: 0.6806 - footwear_output_acc: 0.7194 - pose_output_acc: 0.8596 - emotion_output_acc: 0.7205 - val_loss: 7.1846 - val_gender_output_loss: 0.5049 - val_image_quality_output_loss: 0.9748 - val_age_output_loss: 1.3869 - val_weight_output_loss: 0.9975 - val_bag_output_loss: 0.8824 - val_footwear_output_loss: 0.8370 - val_pose_output_loss: 0.7199 - val_emotion_output_loss: 0.8813 - val_gender_output_acc: 0.7666 - val_image_quality_output_acc: 0.5328 - val_age_output_acc: 0.4002 - val_weight_output_acc: 0.6134 - val_bag_output_acc: 0.5963 - val_footwear_output_acc: 0.6356 - val_pose_output_acc: 0.7117 - val_emotion_output_acc: 0.7122\n",
            "Epoch 18/24\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 5.5421 - gender_output_loss: 0.2449 - image_quality_output_loss: 0.7591 - age_output_loss: 1.2288 - weight_output_loss: 0.8279 - bag_output_loss: 0.7212 - footwear_output_loss: 0.6164 - pose_output_loss: 0.3548 - emotion_output_loss: 0.7888 - gender_output_acc: 0.8924 - image_quality_output_acc: 0.6717 - age_output_acc: 0.4638 - weight_output_acc: 0.6808 - bag_output_acc: 0.6838 - footwear_output_acc: 0.7358 - pose_output_acc: 0.8693 - emotion_output_acc: 0.7228 - val_loss: 7.4593 - val_gender_output_loss: 0.6135 - val_image_quality_output_loss: 0.9650 - val_age_output_loss: 1.4014 - val_weight_output_loss: 1.0256 - val_bag_output_loss: 0.9336 - val_footwear_output_loss: 0.8388 - val_pose_output_loss: 0.7698 - val_emotion_output_loss: 0.9116 - val_gender_output_acc: 0.7293 - val_image_quality_output_acc: 0.5504 - val_age_output_acc: 0.4183 - val_weight_output_acc: 0.6159 - val_bag_output_acc: 0.5801 - val_footwear_output_acc: 0.6366 - val_pose_output_acc: 0.6910 - val_emotion_output_acc: 0.7142\n",
            "Epoch 19/24\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 5.2535 - gender_output_loss: 0.2306 - image_quality_output_loss: 0.7271 - age_output_loss: 1.1787 - weight_output_loss: 0.7913 - bag_output_loss: 0.6812 - footwear_output_loss: 0.5836 - pose_output_loss: 0.3001 - emotion_output_loss: 0.7609 - gender_output_acc: 0.8999 - image_quality_output_acc: 0.6889 - age_output_acc: 0.4868 - weight_output_acc: 0.6909 - bag_output_acc: 0.7046 - footwear_output_acc: 0.7482 - pose_output_acc: 0.8911 - emotion_output_acc: 0.7257 - val_loss: 7.3185 - val_gender_output_loss: 0.5407 - val_image_quality_output_loss: 0.9682 - val_age_output_loss: 1.3876 - val_weight_output_loss: 1.0503 - val_bag_output_loss: 0.8778 - val_footwear_output_loss: 0.8514 - val_pose_output_loss: 0.7582 - val_emotion_output_loss: 0.8842 - val_gender_output_acc: 0.7732 - val_image_quality_output_acc: 0.5544 - val_age_output_acc: 0.4098 - val_weight_output_acc: 0.6179 - val_bag_output_acc: 0.5948 - val_footwear_output_acc: 0.6633 - val_pose_output_acc: 0.7102 - val_emotion_output_acc: 0.7132\n",
            "Epoch 20/24\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 5.0386 - gender_output_loss: 0.2143 - image_quality_output_loss: 0.6903 - age_output_loss: 1.1473 - weight_output_loss: 0.7595 - bag_output_loss: 0.6456 - footwear_output_loss: 0.5606 - pose_output_loss: 0.2753 - emotion_output_loss: 0.7456 - gender_output_acc: 0.9049 - image_quality_output_acc: 0.7032 - age_output_acc: 0.5009 - weight_output_acc: 0.7093 - bag_output_acc: 0.7234 - footwear_output_acc: 0.7577 - pose_output_acc: 0.9005 - emotion_output_acc: 0.7312 - val_loss: 7.4857 - val_gender_output_loss: 0.6364 - val_image_quality_output_loss: 0.9800 - val_age_output_loss: 1.3972 - val_weight_output_loss: 1.0617 - val_bag_output_loss: 0.8782 - val_footwear_output_loss: 0.8693 - val_pose_output_loss: 0.7868 - val_emotion_output_loss: 0.8761 - val_gender_output_acc: 0.7399 - val_image_quality_output_acc: 0.5514 - val_age_output_acc: 0.4128 - val_weight_output_acc: 0.6184 - val_bag_output_acc: 0.6069 - val_footwear_output_acc: 0.6381 - val_pose_output_acc: 0.7182 - val_emotion_output_acc: 0.7142\n",
            "Epoch 21/24\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 4.7237 - gender_output_loss: 0.1852 - image_quality_output_loss: 0.6487 - age_output_loss: 1.0979 - weight_output_loss: 0.7125 - bag_output_loss: 0.6058 - footwear_output_loss: 0.5179 - pose_output_loss: 0.2283 - emotion_output_loss: 0.7274 - gender_output_acc: 0.9225 - image_quality_output_acc: 0.7242 - age_output_acc: 0.5194 - weight_output_acc: 0.7244 - bag_output_acc: 0.7466 - footwear_output_acc: 0.7759 - pose_output_acc: 0.9173 - emotion_output_acc: 0.7394 - val_loss: 7.4888 - val_gender_output_loss: 0.5651 - val_image_quality_output_loss: 0.9899 - val_age_output_loss: 1.4303 - val_weight_output_loss: 1.0398 - val_bag_output_loss: 0.9359 - val_footwear_output_loss: 0.8592 - val_pose_output_loss: 0.7708 - val_emotion_output_loss: 0.8978 - val_gender_output_acc: 0.7626 - val_image_quality_output_acc: 0.5570 - val_age_output_acc: 0.3992 - val_weight_output_acc: 0.6134 - val_bag_output_acc: 0.5872 - val_footwear_output_acc: 0.6431 - val_pose_output_acc: 0.7213 - val_emotion_output_acc: 0.7077\n",
            "Epoch 22/24\n",
            "360/360 [==============================] - 42s 116ms/step - loss: 4.4820 - gender_output_loss: 0.1706 - image_quality_output_loss: 0.6180 - age_output_loss: 1.0499 - weight_output_loss: 0.6818 - bag_output_loss: 0.5646 - footwear_output_loss: 0.4921 - pose_output_loss: 0.2092 - emotion_output_loss: 0.6958 - gender_output_acc: 0.9270 - image_quality_output_acc: 0.7398 - age_output_acc: 0.5489 - weight_output_acc: 0.7339 - bag_output_acc: 0.7602 - footwear_output_acc: 0.7879 - pose_output_acc: 0.9279 - emotion_output_acc: 0.7483 - val_loss: 7.6504 - val_gender_output_loss: 0.6305 - val_image_quality_output_loss: 0.9995 - val_age_output_loss: 1.4505 - val_weight_output_loss: 1.0913 - val_bag_output_loss: 0.9445 - val_footwear_output_loss: 0.8624 - val_pose_output_loss: 0.7794 - val_emotion_output_loss: 0.8923 - val_gender_output_acc: 0.7490 - val_image_quality_output_acc: 0.5524 - val_age_output_acc: 0.4118 - val_weight_output_acc: 0.6184 - val_bag_output_acc: 0.5948 - val_footwear_output_acc: 0.6507 - val_pose_output_acc: 0.7188 - val_emotion_output_acc: 0.7147\n",
            "Epoch 23/24\n",
            "359/360 [============================>.] - ETA: 0s - loss: 4.2690 - gender_output_loss: 0.1574 - image_quality_output_loss: 0.5835 - age_output_loss: 1.0102 - weight_output_loss: 0.6450 - bag_output_loss: 0.5300 - footwear_output_loss: 0.4702 - pose_output_loss: 0.1965 - emotion_output_loss: 0.6761 - gender_output_acc: 0.9323 - image_quality_output_acc: 0.7577 - age_output_acc: 0.5579 - weight_output_acc: 0.7443 - bag_output_acc: 0.7765 - footwear_output_acc: 0.7980 - pose_output_acc: 0.9330 - emotion_output_acc: 0.7553Epoch 23/24\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 4.2702 - gender_output_loss: 0.1577 - image_quality_output_loss: 0.5835 - age_output_loss: 1.0105 - weight_output_loss: 0.6447 - bag_output_loss: 0.5301 - footwear_output_loss: 0.4703 - pose_output_loss: 0.1966 - emotion_output_loss: 0.6767 - gender_output_acc: 0.9324 - image_quality_output_acc: 0.7579 - age_output_acc: 0.5578 - weight_output_acc: 0.7446 - bag_output_acc: 0.7765 - footwear_output_acc: 0.7981 - pose_output_acc: 0.9328 - emotion_output_acc: 0.7548 - val_loss: 7.7110 - val_gender_output_loss: 0.6360 - val_image_quality_output_loss: 0.9903 - val_age_output_loss: 1.4369 - val_weight_output_loss: 1.0920 - val_bag_output_loss: 0.9285 - val_footwear_output_loss: 0.8985 - val_pose_output_loss: 0.8396 - val_emotion_output_loss: 0.8891 - val_gender_output_acc: 0.7611 - val_image_quality_output_acc: 0.5549 - val_age_output_acc: 0.4113 - val_weight_output_acc: 0.6174 - val_bag_output_acc: 0.5917 - val_footwear_output_acc: 0.6401 - val_pose_output_acc: 0.7218 - val_emotion_output_acc: 0.7137\n",
            "Epoch 24/24\n",
            "360/360 [==============================] - 42s 117ms/step - loss: 4.1846 - gender_output_loss: 0.1541 - image_quality_output_loss: 0.5771 - age_output_loss: 0.9906 - weight_output_loss: 0.6329 - bag_output_loss: 0.5204 - footwear_output_loss: 0.4520 - pose_output_loss: 0.1876 - emotion_output_loss: 0.6699 - gender_output_acc: 0.9359 - image_quality_output_acc: 0.7585 - age_output_acc: 0.5697 - weight_output_acc: 0.7494 - bag_output_acc: 0.7800 - footwear_output_acc: 0.8051 - pose_output_acc: 0.9346 - emotion_output_acc: 0.7554 - val_loss: 7.9176 - val_gender_output_loss: 0.7188 - val_image_quality_output_loss: 0.9983 - val_age_output_loss: 1.4763 - val_weight_output_loss: 1.1489 - val_bag_output_loss: 0.9675 - val_footwear_output_loss: 0.9089 - val_pose_output_loss: 0.7912 - val_emotion_output_loss: 0.9077 - val_gender_output_acc: 0.7440 - val_image_quality_output_acc: 0.5615 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6164 - val_bag_output_acc: 0.5963 - val_footwear_output_acc: 0.6452 - val_pose_output_acc: 0.7273 - val_emotion_output_acc: 0.7041\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9385e2e438>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cU8z-vmkUbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history=model.history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvoR3OVIkXSK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9969fd63-5a3d-4db8-b232-fdee080103a4"
      },
      "source": [
        "#print(history.history.keys())\n",
        "for x in history.history.keys():\n",
        "  if x.startswith('val') and x.endswith('acc'):\n",
        "    print(x, \" -- \",  history.history[x][-1])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val_gender_output_acc  --  0.7439516129032258\n",
            "val_image_quality_output_acc  --  0.561491935483871\n",
            "val_age_output_acc  --  0.4027217741935484\n",
            "val_weight_output_acc  --  0.6164314516129032\n",
            "val_bag_output_acc  --  0.5962701612903226\n",
            "val_footwear_output_acc  --  0.6451612903225806\n",
            "val_pose_output_acc  --  0.7273185483870968\n",
            "val_emotion_output_acc  --  0.704133064516129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYs-jEtYlL6y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9229ba07-3f68-4fde-ae6b-92daf9f4e953"
      },
      "source": [
        "val_loss=[]\n",
        "for i in range(24):\n",
        "  sum=0\n",
        "  l = ['val_gender_output_loss','val_image_quality_output_loss',  'val_age_output_loss', 'val_weight_output_loss', 'val_bag_output_loss', 'val_footwear_output_loss', 'val_pose_output_loss',\n",
        " 'val_emotion_output_loss']\n",
        "  for x in l:\n",
        "    sum += history.history[x][i]\n",
        "  val_loss.append(sum)\n",
        "print(val_loss)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7.822822791914786, 7.76443543934053, 7.886516252833029, 7.667036883292659, 7.674166737064239, 7.635651042384486, 7.498272147870834, 7.412837890848036, 7.284262038046315, 7.337024938675665, 7.249019652605057, 7.361298317870786, 7.349039598818749, 7.15071385425906, 7.395078060127075, 7.409209105276292, 7.184613381662677, 7.459289085480474, 7.318483590118347, 7.485670234887832, 7.488770798329385, 7.650404232163583, 7.710980749899341, 7.917625443589302]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM_4uT8qlUKF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "8496cdfd-c706-4674-98ab-95c299bed10c"
      },
      "source": [
        "  import matplotlib.pyplot as plt\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(val_loss)\n",
        "  plt.title('model loss')\n",
        "  plt.ylabel('loss')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train', 'validation'], loc='upper left')\n",
        "  plt.show()  #no reduction in validation loss"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}